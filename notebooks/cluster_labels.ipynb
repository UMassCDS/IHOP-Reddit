{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import operator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Plotly graphs have more features than seaborn, like interactive hover text & zoom, but they don't show up in pdfs\n",
    "import plotly.express as px\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import ihop\n",
    "\n",
    "import ihop.community2vec as ic2v\n",
    "import ihop.import_data as iid\n",
    "import ihop.text_processing as itp\n",
    "import ihop.clustering as ic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2V_MODEL_PATH = \"../data/community2vec/RC_2021-05/best_model/keyedVectors\"\n",
    "\n",
    "# This data was produced by the bagOfWords_preprocessing_databricks.ipynb notbook, it removes deleted comments/submissions and comments from top most commenting users, joins comments and submissions, but has no text preprocessing\n",
    "# These are essentially the same steps as ihop.import_data bow \n",
    "REDDIT_THREADS_PATH = \"../data/bagOfWords/2021-05_to_2021-06_joined_submissions_comments_5percentTopUsersExcludedFromComments_02102022.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 15:13:39,055 : WARNING : WARNING: No HADOOP_HOME variable found, zstd decompression may not be available\n",
      "22/06/24 15:13:39 WARN Utils: Your hostname, virginia-beastbox resolves to a loopback address: 127.0.1.1; using 10.3.40.174 instead (on interface wlp147s0)\n",
      "22/06/24 15:13:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/24 15:13:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-06-24 15:13:41,565 : INFO : Spark configuration: [('spark.app.name', 'Cluster Labels Notebook'), ('spark.driver.memory', '36G'), ('spark.driver.port', '37491'), ('spark.executor.id', 'driver'), ('spark.driver.memoryOverhead', '8G'), ('spark.driver.host', 'wpa174.wpanet.cs.local'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.sql.warehouse.dir', 'file:/home/virginia/workspace/IHOP/notebooks/spark-warehouse'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.startTime', '1656098020104'), ('spark.app.id', 'local-1656098020736')]\n"
     ]
    }
   ],
   "source": [
    "spark = ihop.utils.get_spark_session(\"Cluster Labels Notebook\",config={\"spark.driver.memory\":\"36G\", \"spark.driver.memoryOverhead\":\"8G\", \"spark.master\":\"local[*]\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.read.parquet(REDDIT_THREADS_PATH)#.limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subreddit',\n",
       " 'author',\n",
       " 'created_utc',\n",
       " 'id',\n",
       " 'score',\n",
       " 'selftext',\n",
       " 'title',\n",
       " 'url',\n",
       " 'fullname_id',\n",
       " 'comments_subreddit',\n",
       " 'comments_id',\n",
       " 'parent_id',\n",
       " 'comments_score',\n",
       " 'link_id',\n",
       " 'comments_author',\n",
       " 'body',\n",
       " 'comments_created_utc',\n",
       " 'time_to_comment_in_seconds']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----------+------+-----+--------------------+--------------------+--------------------+-----------+-------------------+-----------+----------+--------------+---------+--------------------+--------------------+--------------------+--------------------------+\n",
      "|          subreddit|     author|created_utc|    id|score|            selftext|               title|                 url|fullname_id| comments_subreddit|comments_id| parent_id|comments_score|  link_id|     comments_author|                body|comments_created_utc|time_to_comment_in_seconds|\n",
      "+-------------------+-----------+-----------+------+-----+--------------------+--------------------+--------------------+-----------+-------------------+-----------+----------+--------------+---------+--------------------+--------------------+--------------------+--------------------------+\n",
      "|        superleague|  SL_Thread| 1619960426|n135uh|    7|||SUNDAY||\\n|:--|...|Sunday Match Thre...|https://www.reddi...|  t3_n135uh|        superleague|    gwoa7gy| t3_n135uh|             1|t3_n135uh|        longaltifrog|Cracking game , n...|          1619970474|                   10048.0|\n",
      "|        superleague|  SL_Thread| 1619960426|n135uh|    7|||SUNDAY||\\n|:--|...|Sunday Match Thre...|https://www.reddi...|  t3_n135uh|        superleague|    gwoap20| t3_n135uh|             2|t3_n135uh|      TepidBojangles|Pretty surprised ...|          1619970665|                   10239.0|\n",
      "|       MuslumanTurk|TurkishGuys| 1619827251|n26ps0|   18|uzun süredir aklı...|Neden Gayrimüslim...|https://www.reddi...|  t3_n26ps0|       MuslumanTurk|    gwiwbf3|t1_gwiumn7|             2|t3_n26ps0|          Kesmeseker|Yobaz genellemesi...|          1619861406|                   34155.0|\n",
      "|       MuslumanTurk|TurkishGuys| 1619827251|n26ps0|   18|uzun süredir aklı...|Neden Gayrimüslim...|https://www.reddi...|  t3_n26ps0|       MuslumanTurk|    gwiumn7|t1_gwiu1f5|             1|t3_n26ps0|pandoraninbirakutusu|Yobaz mısın dostu...|          1619859701|                   32450.0|\n",
      "|       MuslumanTurk|TurkishGuys| 1619827251|n26ps0|   18|uzun süredir aklı...|Neden Gayrimüslim...|https://www.reddi...|  t3_n26ps0|       MuslumanTurk|    gwiwenw|t1_gwiwbf3|             1|t3_n26ps0|pandoraninbirakutusu|Müslümanların hep...|          1619861498|                   34247.0|\n",
      "|       MuslumanTurk|TurkishGuys| 1619827251|n26ps0|   18|uzun süredir aklı...|Neden Gayrimüslim...|https://www.reddi...|  t3_n26ps0|       MuslumanTurk|    gwj71ze|t1_gwiu1f5|             2|t3_n26ps0|my_cakeday_izma_bday|                  R2|          1619871012|                   43761.0|\n",
      "|relationship_advice|  P_Mankuly| 1619827472|n26sc8|    2|As title says, my...|My wife lost her ...|https://www.reddi...|  t3_n26sc8|relationship_advice|    gwhn7xc| t3_n26sc8|             8|t3_n26sc8|            oops3719|You have to under...|          1619829228|                    1756.0|\n",
      "|relationship_advice|  P_Mankuly| 1619827472|n26sc8|    2|As title says, my...|My wife lost her ...|https://www.reddi...|  t3_n26sc8|relationship_advice|    gwkp2on|t1_gwj4411|             1|t3_n26sc8|           P_Mankuly|I decided only to...|          1619896462|                   68990.0|\n",
      "|relationship_advice|  P_Mankuly| 1619827472|n26sc8|    2|As title says, my...|My wife lost her ...|https://www.reddi...|  t3_n26sc8|relationship_advice|    gwkp9da|t1_gwkitxa|             1|t3_n26sc8|           P_Mankuly|I'm not just call...|          1619896545|                   69073.0|\n",
      "|relationship_advice|  P_Mankuly| 1619827472|n26sc8|    2|As title says, my...|My wife lost her ...|https://www.reddi...|  t3_n26sc8|relationship_advice|    gwkr7ld|t1_gwkiq5l|             1|t3_n26sc8|           P_Mankuly|I agree on that, ...|          1619897424|                   69952.0|\n",
      "+-------------------+-----------+-----------+------+-----+--------------------+--------------------+--------------------+-----------+-------------------+-----------+----------+--------------+---------+--------------------+--------------------+--------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'subreddit', 'document_text']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproduce some of the work from ihop.text_processing.py to concat submission and comments within a time range of 3s-3d, but don't use TF-IDF \n",
    "corpus = itp.SparkCorpus.init_from_joined_dataframe(dataframe, max_time_delta=60*60*72, min_time_delta=3)\n",
    "corpus.document_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_text_dataframe = corpus.document_dataframe.groupBy(\"subreddit\").agg(\n",
    "    fn.concat_ws(\" \", fn.collect_list(\"document_text\")).alias(\"subreddit_text\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(subreddit_text_dataframe.rdd.getNumPartitions())\n",
    "subreddit_text_dataframe = subreddit_text_dataframe.repartition(10000)\n",
    "#print(subreddit_text_dataframe.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 15:13:44,777 : INFO : Parameters for SparkTextPreprocessingPipeline: {'self': <ihop.text_processing.SparkTextPreprocessingPipeline object at 0x7f62417cdb20>, 'input_col': 'subreddit_text', 'output_col': 'vectorized', 'tokens_col': 'tokenized', 'filtered_tokens_col': 'tokensNoStopWords', 'tokenization_pattern': '([\\\\p{L}\\\\p{N}#@][\\\\p{L}\\\\p{N}\\\\p{Pd}\\\\p{Pc}\\\\p{S}\\\\p{P}]*[\\\\p{L}\\\\p{N}])|[\\\\p{L}\\\\p{N}]|[^\\\\p{P}\\\\s]', 'match_gaps': False, 'toLowercase': True, 'stopLanguage': 'english', 'stopCaseSensitive': False, 'maxDF': 10000, 'minDF': 0.0, 'minTF': 5, 'binary': False, 'useIDF': False}\n",
      "2022-06-24 15:13:44,793 : INFO : Using RegexTokenizer with following parameters: {inputCol: subreddit_text, outputCol: tokenized, pattern: ([\\p{L}\\p{N}#@][\\p{L}\\p{N}\\p{Pd}\\p{Pc}\\p{S}\\p{P}]*[\\p{L}\\p{N}])|[\\p{L}\\p{N}]|[^\\p{P}\\s], toLowercase: True, gaps: False}\n",
      "2022-06-24 15:13:44,835 : INFO : Using StopWordsRemover with the following parameters: {inputCol: tokenized, outputCol: tokensNoStopWords, stopWords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would'], caseSensitive: False}\n",
      "2022-06-24 15:13:44,842 : INFO : Using CountVectorizer with following parameters: {inputCol: tokensNoStopWords, outputCol: vectorized, minDF: 0.0, maxDF: 10000.0, minTF: 5.0, vocabSize: 262144}\n",
      "2022-06-24 15:13:44,842 : INFO : Fitting SparkTextPreprocessingPipeline\n",
      "[Stage 5:===================================================>  (190 + 10) / 200]\r"
     ]
    }
   ],
   "source": [
    "text_pipeline = itp.SparkTextPreprocessingPipeline(input_col = \"subreddit_text\", maxDF=10000, minDF=0.0, minTF=5)\n",
    "subreddit_vectorized_df = text_pipeline.fit_transform(subreddit_text_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_vectorized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the dataframe into a pandas sparse vectors into numpy arrays\n",
    "pandas_df = subreddit_vectorized_df.select(\"subreddit\", \"vectorized\").toPandas()\n",
    "id_to_term = text_pipeline.get_id_to_word()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df[\"numpy_vectorized\"] = pandas_df[\"vectorized\"].apply(lambda x: x.toArray())\n",
    "pandas_df = pandas_df.drop(\"vectorized\", axis=1)\n",
    "\n",
    "display(pandas_df.head(10))\n",
    "pandas_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corpus_counts = np.sum(pandas_df[\"numpy_vectorized\"])\n",
    "print(\"Array transform to numpy:\", total_corpus_counts)\n",
    "print(\"Vocab size check:\", len(total_corpus_counts))\n",
    "total_tokens = np.sum(total_corpus_counts)\n",
    "print(\"Total tokens in corpus:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_probabilities(token_count_pdf, vectorized_col):\n",
    "    token_count_array = np.sum(token_count_pdf[vectorized_col])\n",
    "    total_tokens = np.sum(token_count_array)\n",
    "    return token_count_array/total_tokens\n",
    "\n",
    "def compute_pmi(token_count_pdf, vectorized_col, total_term_probabilities):\n",
    "    \"\"\"Returns numpy array storing pointwise mutual information between given dataframe values and the overall corpus.\n",
    "\n",
    "    :param token_count_pdf: _description_\n",
    "    :param vectorized_col: \n",
    "    :param total_term_probabilities: _description_\n",
    "    \"\"\"\n",
    "    conditional_probs = compute_token_probabilities(token_count_pdf, vectorized_col)\n",
    "    pmis = np.log2(conditional_probs / total_term_probabilities)\n",
    "    return pmis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_term_probabilities = compute_token_probabilities(pandas_df, \"numpy_vectorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_subreddits = [\"4chan\", \"Utah\", \"MensRights\", \"conservatives\", \"libertarianmemes\"]\n",
    "k = 5\n",
    "for s in selected_subreddits:\n",
    " \n",
    "    selected_pdf = pandas_df[pandas_df[\"subreddit\"]==s]\n",
    "    display(selected_pdf)\n",
    "\n",
    "    pmi_values = compute_pmi(selected_pdf, \"numpy_vectorized\", corpus_term_probabilities)\n",
    "\n",
    "    top_pmi_indices = np.argpartition(pmi_values, -k)[-k:]\n",
    "    print(\"Top PMI values for subreddit:\", s)\n",
    "    print(\"\\tTerm\\tPMI\")\n",
    "    for i in top_pmi_indices:\n",
    "        print(f\"\\t{id_to_term[i]}\\t{pmi_values[i]}\")\n",
    "    print()\n",
    "    bottom_pmi_indices = np.argpartition(pmi_values,k)[:k]\n",
    "    print(\"Bottom PMI values for subreddit:\", s)\n",
    "    print(\"\\tTerm\\tPMI\")\n",
    "    for i in bottom_pmi_indices:\n",
    "        print(f\"\\t{id_to_term[i]}\\t{pmi_values[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "645e6807a54f1780e2a687ae083eb2a94c3bf70f4809b5f746590e8ef179a45c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
