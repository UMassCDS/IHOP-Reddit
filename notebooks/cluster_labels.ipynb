{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import operator\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Plotly graphs have more features than seaborn, like interactive hover text & zoom, but they don't show up in pdfs\n",
    "import plotly.express as px\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import ihop\n",
    "\n",
    "import ihop.community2vec as ic2v\n",
    "import ihop.import_data as iid\n",
    "import ihop.text_processing as itp\n",
    "import ihop.clustering as ic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2V_MODEL_PATH = \"../data/community2vec/RC_2021-05/best_model/keyedVectors\"\n",
    "\n",
    "# This data was produced by the bagOfWords_preprocessing_databricks.ipynb notbook, it removes deleted comments/submissions and comments from top most commenting users, joins comments and submissions, but has no text preprocessing\n",
    "# These are essentially the same steps as ihop.import_data bow \n",
    "REDDIT_THREADS_PATH = \"../data/bagOfWords/2021-05_to_2021-06_joined_submissions_comments_5percentTopUsersExcludedFromComments_02102022.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 12:29:29,123 : WARNING : WARNING: No HADOOP_HOME variable found, zstd decompression may not be available\n",
      "22/06/27 12:29:29 WARN Utils: Your hostname, virginia-beastbox resolves to a loopback address: 127.0.1.1; using 10.3.40.174 instead (on interface wlp147s0)\n",
      "22/06/27 12:29:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/27 12:29:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/27 12:29:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-06-27 12:29:31,948 : INFO : Spark configuration: [('spark.app.name', 'Cluster Labels Notebook'), ('spark.app.startTime', '1656347370477'), ('spark.driver.port', '42613'), ('spark.driver.memory', '36G'), ('spark.app.id', 'local-1656347371121'), ('spark.executor.id', 'driver'), ('spark.driver.host', 'wpa174.wpanet.cs.local'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.sql.warehouse.dir', 'file:/home/virginia/workspace/IHOP/notebooks/spark-warehouse'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "spark = ihop.utils.get_spark_session(\"Cluster Labels Notebook\",config={\"spark.driver.memory\":\"36G\", \"spark.master\":\"local[*]\"})#, \"spark.driver.extraJavaOptions\":\"-XX:+UseG1GC\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.read.parquet(REDDIT_THREADS_PATH)#.limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subreddit',\n",
       " 'author',\n",
       " 'created_utc',\n",
       " 'id',\n",
       " 'score',\n",
       " 'selftext',\n",
       " 'title',\n",
       " 'url',\n",
       " 'fullname_id',\n",
       " 'comments_subreddit',\n",
       " 'comments_id',\n",
       " 'parent_id',\n",
       " 'comments_score',\n",
       " 'link_id',\n",
       " 'comments_author',\n",
       " 'body',\n",
       " 'comments_created_utc',\n",
       " 'time_to_comment_in_seconds']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(subreddit='superleague', author='SL_Thread', created_utc='1619960426', id='n135uh', score=7, selftext=\"||SUNDAY||\\n|:--|:--:|--:|\\n||15:00\\n|[](#game-hudds)|[](#vs)|[](#game-leeds)\\n||JOHN SMITH'S STADIUM\\n||[OurLeague Stream](/r/OL)\\n___\\n\\n**Remember to comment your thoughts below and upvote the thread!**\", title='Sunday Match Thread | Round Five', url='https://www.reddit.com/r/superleague/comments/n135uh/sunday_match_thread_round_five/', fullname_id='t3_n135uh', comments_subreddit='superleague', comments_id='gwoa7gy', parent_id='t3_n135uh', comments_score=1, link_id='t3_n135uh', comments_author='longaltifrog', body='Cracking game , nerves were going at the end', comments_created_utc=1619970474, time_to_comment_in_seconds=10048.0)]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'subreddit', 'document_text']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproduce some of the work from ihop.text_processing.py to concat submission and comments within a time range of 3s-3d, but don't use TF-IDF \n",
    "corpus = itp.SparkCorpus.init_from_joined_dataframe(dataframe, max_time_delta=60*60*72, min_time_delta=3)\n",
    "corpus.document_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 12:29:34,910 : INFO : Parameters for SparkTextPreprocessingPipeline: {'self': <ihop.text_processing.SparkTextPreprocessingPipeline object at 0x7f7a1e0098b0>, 'input_col': 'document_text', 'output_col': 'vectorized', 'tokens_col': 'tokenized', 'filtered_tokens_col': 'tokensNoStopWords', 'tokenization_pattern': '([\\\\p{L}\\\\p{N}#@][\\\\p{L}\\\\p{N}\\\\p{Pd}\\\\p{Pc}\\\\p{S}\\\\p{P}]*[\\\\p{L}\\\\p{N}])|[\\\\p{L}\\\\p{N}]|[^\\\\p{P}\\\\s]', 'match_gaps': False, 'toLowercase': True, 'stopLanguage': 'english', 'stopCaseSensitive': False, 'maxDF': 9223372036854775807, 'minDF': 1.0, 'minTF': 0.0, 'binary': False, 'useIDF': False}\n",
      "2022-06-27 12:29:34,930 : INFO : Using RegexTokenizer with following parameters: {inputCol: document_text, outputCol: tokenized, pattern: ([\\p{L}\\p{N}#@][\\p{L}\\p{N}\\p{Pd}\\p{Pc}\\p{S}\\p{P}]*[\\p{L}\\p{N}])|[\\p{L}\\p{N}]|[^\\p{P}\\s], toLowercase: True, gaps: False}\n",
      "2022-06-27 12:29:34,990 : INFO : Using StopWordsRemover with the following parameters: {inputCol: tokenized, outputCol: tokensNoStopWords, stopWords: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would'], caseSensitive: False}\n",
      "2022-06-27 12:29:34,997 : INFO : Using CountVectorizer with following parameters: {inputCol: tokensNoStopWords, outputCol: vectorized, minDF: 1.0, maxDF: 9.223372036854776e+18, minTF: 0.0, vocabSize: 262144}\n",
      "2022-06-27 12:29:34,998 : INFO : Fitting SparkTextPreprocessingPipeline\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Spark pipeline, let's try to keep all terms\n",
    "text_pipeline = itp.SparkTextPreprocessingPipeline(input_col = \"document_text\", maxDF=9223372036854775807, minDF=1.0)\n",
    "subreddit_term_freq_df = text_pipeline.fit_transform(corpus.document_dataframe).drop(\"tokenized\", \"document_text\", \"tokensNoStopWords\").groupBy(\"subreddit\").agg(\n",
    "    Summarizer.sum(fn.col(\"vectorized\")).alias(\"sum_vectorized\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144\n",
      "[(0, 'like'), (1, 'get'), (2, 'one'), (3, 'people'), (4, 'think'), (5, 'time'), (6, 'know'), (7, 'good'), (8, 'really'), (9, 'also'), (10, 'even'), (11, 'much'), (12, 'want'), (13, 'see'), (14, 'it’s'), (15, 'i’m'), (16, 'make'), (17, 'still'), (18, 'got'), (19, 'go')]\n"
     ]
    }
   ],
   "source": [
    "# Save vocabulary\n",
    "id_to_term = text_pipeline.get_id_to_word()\n",
    "print(len(id_to_term))\n",
    "print(list(id_to_term.items())[0:20])\n",
    "with open(\"id_to_term.joblib\", 'wb') as f:\n",
    "    joblib.dump(id_to_term, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/27 12:46:41 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/06/27 14:12:47 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "22/06/27 14:12:48 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "[Stage 24:=========>                                            (37 + 16) / 200]\r"
     ]
    }
   ],
   "source": [
    "pandas_df = subreddit_term_freq_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit           object\n",
       "numpy_vectorized    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df[\"numpy_vectorized\"] = pandas_df[\"sum_vectorized\"].apply(lambda x: x.toArray())\n",
    "pandas_df = pandas_df.drop(\"sum_vectorized\", axis=1)\n",
    "pandas_df.to_pickle(\"subreddit_term_freq_pandas.pkl.bz2\")\n",
    "pandas_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array transform to numpy: [17171465. 11455174. 10660641.  8591402.  8150215.  7856228.  7642621.\n",
      "  7435428.  7007795.  6909148.  6223672.  5817422.  5642260.  5513149.\n",
      "  5508071.  5328943.  5257921.  5239334.  5174020.  5099136.  5021278.\n",
      "  4522704.  4478883.  4441071.  4436014.  4301082.  4231464.  4169873.\n",
      "  4090440.  3912745.  3906303.  3829548.  3822345.  3771797.  3743472.\n",
      "  3738377.  3619695.  3588361.  3574734.  3536373.  3518774.  3506830.\n",
      "  3484572.  3410769.  3347400.  3240375.  3108731.  3102848.  3054888.\n",
      "  3038292.  3008993.  3000398.  2998319.  2959405.  2899682.  2895671.\n",
      "  2890123.  2866146.  2852069.  2851592.  2817379.  2783793.  2776669.\n",
      "  2759524.  2724603.  2712428.  2707624.  2704421.  2672268.  2668705.\n",
      "  2615804.  2558879.  2556918.  2542517.  2535886.  2520314.  2489178.\n",
      "  2482391.  2440037.  2424375.  2421008.  2407372.  2398398.  2360801.\n",
      "  2339463.  2289051.  2283343.  2281059.  2280545.  2277136.  2218185.\n",
      "  2137211.  2126091.  2123373.  2114481.  2086930.  2027323.  1999315.\n",
      "  1998918.  1989452.  1989258.  1965738.  1960328.  1946515.  1939279.\n",
      "  1928477.  1924728.  1923765.  1918977.  1907928.  1904228.  1903863.\n",
      "  1901391.  1855648.  1847148.  1842978.  1838227.  1837862.  1823306.\n",
      "  1802908.  1800136.  1779992.  1775481.  1772915.  1754367.  1742965.\n",
      "  1728360.  1725470.  1725326.  1717901.  1717728.  1701080.  1688738.\n",
      "  1677859.  1666383.  1646838.  1646786.  1627622.  1624710.  1591615.\n",
      "  1580870.  1574427.  1548581.  1536128.  1529864.  1527032.  1522973.\n",
      "  1521306.  1511455.  1496717.  1486480.  1485260.  1485092.  1483755.\n",
      "  1475762.  1472584.  1468722.  1464537.  1461720.  1458703.  1448422.\n",
      "  1447533.  1445477.  1444842.  1444218.  1441923.  1426968.  1423995.\n",
      "  1422005.  1410464.  1406660.  1405687.  1404975.  1399116.  1398794.\n",
      "  1372950.  1365432.  1364628.  1355123.  1349011.  1348992.  1344031.\n",
      "  1339642.  1334630.  1334610.  1315855.  1314589.  1312505.  1309499.\n",
      "  1308454.  1304948.  1294711.  1289797.  1287178.  1280547.  1257194.\n",
      "  1255590.  1254306.  1252615.  1249245.  1248294.  1245430.  1234454.\n",
      "  1234344.  1222640.  1219070.  1218052.  1206872.  1204573.  1200493.\n",
      "  1192392.  1187675.  1185782.  1183458.  1179290.  1177850.  1173197.\n",
      "  1172031.  1166237.  1162622.  1158773.  1154328.  1152519.  1151548.\n",
      "  1149595.  1142533.  1141618.  1140924.  1135556.  1134258.  1131979.\n",
      "  1130631.  1130559.  1130241.  1124824.  1117707.  1115669.  1112368.\n",
      "  1109457.  1107753.  1101549.  1099623.  1097671.  1087276.  1087084.\n",
      "  1082068.  1070456.  1070224.  1065983.  1064313.  1063949.  1061842.\n",
      "  1061647.  1049989.  1047753.  1044728.  1043888.  1042244.  1030301.\n",
      "  1022344.  1021513.  1020860.  1016647.  1015257.  1012726.  1011990.\n",
      "  1005645.  1001785.  1000105.   997803.   989331.   981988.   981038.\n",
      "   977489.   975098.   970456.   969807.   968474.   966994.   964987.\n",
      "   961287.   961044.   950969.   950710.   950530.   946168.   941942.\n",
      "   939245.   935172.   934542.   934048.   931545.   930601.   929050.\n",
      "   927630.   927278.   927172.   924395.   918075.   915289.   908412.\n",
      "   908278.   907518.   903225.   902399.   900280.   898222.   894917.\n",
      "   893502.   888223.   886781.   885631.   881872.   880696.   878213.\n",
      "   875102.   874040.   872647.   872200.   870932.   868613.   867402.\n",
      "   865357.   857117.   854457.   853512.   851420.   848456.   846841.\n",
      "   842218.   841257.   840703.   838759.   836878.   832914.   832214.\n",
      "   831492.   830938.   829691.   828669.   827897.   824508.   819345.\n",
      "   815206.   812025.   808114.   806257.   804551.   797235.   795934.\n",
      "   792765.   792538.   782544.   775194.   774570.   773960.   773955.\n",
      "   771290.   768154.   763949.   762499.   761493.   757011.   753672.\n",
      "   753533.   751526.   750142.   748200.   746586.   744431.   744308.\n",
      "   744006.   743663.   742752.   741318.   740500.   737437.   737073.\n",
      "   736788.   735680.   733419.   733373.   733060.   732515.   730520.\n",
      "   730182.   726774.   726185.   722600.   721016.   720801.   720773.\n",
      "   720129.   719890.   719419.   718362.   715769.   706218.   704682.\n",
      "   704199.   697551.   696809.   695442.   695266.   693772.   690597.\n",
      "   689601.   687620.   685582.   681313.   680847.   680703.   679894.\n",
      "   679482.   677238.   677229.   675934.   672482.   672200.   671917.\n",
      "   671855.   670899.   669958.   669332.   668793.   668166.   667040.\n",
      "   666854.   666426.   665219.   662371.   661432.   659634.   659065.\n",
      "   658678.   657467.   656147.   656017.   655646.   654231.   653370.\n",
      "   652707.   652196.   651370.   648976.   648189.   647711.   646187.\n",
      "   642822.   638573.   635746.   633676.   630798.   630155.   629900.\n",
      "   627646.   623124.   622696.   622371.   619505.   619298.   619130.\n",
      "   617208.   614963.   612188.   608896.   608780.   607898.   607555.\n",
      "   607327.   607167.   607090.   606915.   606456.   606126.   606108.\n",
      "   604544.   602181.   600214.   597272.   595927.   595839.   595596.\n",
      "   594304.   588719.   588573.   588170.   587864.   587344.   586762.\n",
      "   586677.   586498.   585256.   584943.   583927.   583828.   583073.\n",
      "   582623.   581784.   581292.   579794.   578923.   577999.   574088.\n",
      "   573314.   572035.   571647.   571244.   569530.   569195.   565795.\n",
      "   565605.   565518.   563762.   562028.   561855.   559371.   558630.\n",
      "   557984.   556378.   555142.   554836.   554622.   552123.   551035.\n",
      "   549270.   547873.   544600.   544415.   541863.   541489.   540599.\n",
      "   540582.   539627.   539494.   537666.   535434.   535226.   534334.\n",
      "   532331.   530345.   529160.   528979.   527923.   526751.   524535.\n",
      "   524178.   522535.   522122.   521824.   520765.   520652.   518689.\n",
      "   516176.   515838.   515831.   513825.   509226.   505183.   504079.\n",
      "   502943.   502524.   500960.   498474.   498162.   494008.   493603.\n",
      "   493187.   490212.   488985.   487857.   487702.   487168.   486912.\n",
      "   485196.   483674.   482425.   481102.   480186.   479193.   478675.\n",
      "   478539.   477879.   477500.   477408.   476862.   476456.   476173.\n",
      "   475859.   475678.   475447.   475190.   474790.   473178.   470850.\n",
      "   469998.   469641.   469563.   469023.   468910.   468525.   466782.\n",
      "   466014.   465760.   464739.   462467.   462006.   461813.   461072.\n",
      "   461048.   460053.   455662.   455359.   454388.   454089.   452995.\n",
      "   450958.   450393.   449685.   448006.   445947.   444992.   444554.\n",
      "   444235.   443414.   442304.   441952.   441704.   441234.   439070.\n",
      "   438549.   437812.   437769.   436468.   436455.   435805.   433454.\n",
      "   433214.   432968.   432460.   431651.   428635.   428516.   428071.\n",
      "   426672.   425628.   425031.   423904.   422998.   422910.   422299.\n",
      "   418071.   417593.   416852.   416439.   415412.   415159.   413489.\n",
      "   412650.   412458.   411779.   409603.   409342.   409304.   408426.\n",
      "   408053.   405615.   404258.   403606.   403078.   402701.   402617.\n",
      "   400573.   399989.   399155.   399067.   396580.   395974.   395848.\n",
      "   394698.   389406.   388514.   388106.   385634.   383892.   383588.\n",
      "   383102.   383073.   380261.   378927.   378187.   375968.   373401.\n",
      "   372244.   367276.   361586.   358295.   347581.   346844.]\n",
      "Vocab size check: 699\n",
      "Total tokens in corpus: 887052777.0\n"
     ]
    }
   ],
   "source": [
    "total_corpus_counts = np.sum(pandas_df[\"numpy_vectorized\"])\n",
    "#print(\"Array transform to numpy:\", total_corpus_counts)\n",
    "print(\"Vocab size check:\", len(total_corpus_counts))\n",
    "total_tokens = np.sum(total_corpus_counts)\n",
    "print(\"Total tokens in corpus:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_probabilities(token_count_pdf, vectorized_col):\n",
    "    token_count_array = np.sum(token_count_pdf[vectorized_col])\n",
    "    total_tokens = np.sum(token_count_array)\n",
    "    return token_count_array/total_tokens\n",
    "\n",
    "def compute_pmi(token_count_pdf, vectorized_col, total_term_probabilities):\n",
    "    \"\"\"Returns numpy array storing pointwise mutual information between given dataframe values and the overall corpus.\n",
    "\n",
    "    :param token_count_pdf: _description_\n",
    "    :param vectorized_col: \n",
    "    :param total_term_probabilities: _description_\n",
    "    \"\"\"\n",
    "    # Compute P(t|class), the conditional proability of the term given class (subreddit, cluster)\n",
    "    conditional_probs = compute_token_probabilities(token_count_pdf, vectorized_col)\n",
    "    pmis = np.log2(conditional_probs / total_term_probabilities)\n",
    "    return pmis\n",
    "\n",
    "def compute_differential_cluster_label_scheme(token_count_pdf, vectorized_col, total_term_probabilities):\n",
    "    \"\"\"Returns Popescul and Ungars method for cluster labeling given dataframe values and the overall corpus.\n",
    "\n",
    "    :param token_count_pdf: _description_\n",
    "    :param vectorized_col: \n",
    "    :param total_term_probabilities: _description_\n",
    "    \"\"\"\n",
    "    # Compute P(t|class), the conditional proability of the term given class (subreddit, cluster)\n",
    "    conditional_t_given_class = compute_token_probabilities(token_count_pdf, vectorized_col)\n",
    "    score_t_given_class = np.square(conditional_t_given_class) / total_term_probabilities\n",
    "    return score_t_given_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes P(t), the probability of the term in the corpus overall\n",
    "corpus_term_probabilities = compute_token_probabilities(pandas_df, \"numpy_vectorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>4chan</td>\n",
       "      <td>[3900.000000000002, 2175.0000000000005, 2094.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                   numpy_vectorized\n",
       "631     4chan  [3900.000000000002, 2175.0000000000005, 2094.0..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top PMI values for subreddit: 4chan\n",
      "\tTerm\tPMI\n",
      "\tfunny\t1.6326218445828053\n",
      "\tcountry\t1.7575841800248684\n",
      "\tgt\t2.0065807731614864\n",
      "\tlmao\t1.8857887970134803\n",
      "\tword\t1.6571239294465787\n",
      "\twhite\t2.074934736050337\n",
      "\tfucking\t2.4702749726810627\n",
      "\tfuck\t2.117571886355602\n",
      "\tbased\t2.669335834035875\n",
      "\tshit\t2.1022675090851637\n",
      "\n",
      "Bottom PMI values for subreddit: 4chan\n",
      "\tTerm\tPMI\n",
      "\tplayers\t-3.930038806298334\n",
      "\tplayer\t-2.871965964359553\n",
      "\t😍\t-4.081181041706523\n",
      "\tupdate\t-2.4540154769062417\n",
      "\twondering\t-2.016417336036202\n",
      "\thi\t-2.837538985167364\n",
      "\tteam\t-2.240384651770118\n",
      "\thoping\t-2.0931878082689606\n",
      "\tseason\t-2.3715517150795935\n",
      "\tdamage\t-2.288096331473424\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>4chan</td>\n",
       "      <td>[3900.000000000002, 2175.0000000000005, 2094.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                   numpy_vectorized\n",
       "631     4chan  [3900.000000000002, 2175.0000000000005, 2094.0..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Popescul Ungars values for subreddit: 4chan\n",
      "\tTerm\tPopescul Ungars\n",
      "\tone\t0.011962367209359764\n",
      "\tget\t0.012010578497592081\n",
      "\twhite\t0.015488217530574892\n",
      "\tgt\t0.019373224443915812\n",
      "\tshit\t0.03830556296804033\n",
      "\tfuck\t0.029843137516142204\n",
      "\tbased\t0.03992174586372757\n",
      "\tpeople\t0.04258978327868785\n",
      "\tfucking\t0.039187056121834554\n",
      "\tlike\t0.025761370211717018\n",
      "\n",
      "Bottom Popescul Ungars values for subreddit: 4chan\n",
      "\tTerm\tPopescul Ungars\n",
      "\thoping\t2.549705971380445e-05\n",
      "\tupdate\t1.88100111808186e-05\n",
      "\tplayer\t1.5643391374097758e-05\n",
      "\tdamage\t3.3998611912131183e-05\n",
      "\tplayers\t4.5078608141616054e-06\n",
      "\twondering\t3.396705987300427e-05\n",
      "\t😍\t2.0297481585038696e-06\n",
      "\thi\t1.361800188460335e-05\n",
      "\tseason\t3.6513591651925655e-05\n",
      "\tadded\t3.967909996850566e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Utah</td>\n",
       "      <td>[673.0000000000001, 540.0000000000003, 454.999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                   numpy_vectorized\n",
       "109      Utah  [673.0000000000001, 540.0000000000003, 454.999..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top PMI values for subreddit: Utah\n",
      "\tTerm\tPMI\n",
      "\tlots\t1.414961403592092\n",
      "\tlocal\t1.5446260921305002\n",
      "\tmoving\t1.598783063882577\n",
      "\tarea\t2.399649213948196\n",
      "\tbeautiful\t1.7456348435486668\n",
      "\tstate\t3.1372640660810633\n",
      "\tcity\t2.841698681735163\n",
      "\tdrive\t2.4925070759879273\n",
      "\tnear\t1.8385111752859693\n",
      "\twater\t2.7434189391465176\n",
      "\n",
      "Bottom PMI values for subreddit: Utah\n",
      "\tTerm\tPMI\n",
      "\tvideos\t-inf\n",
      "\tplayers\t-inf\n",
      "\tplayer\t-inf\n",
      "\tupdate\t-4.463517109682767\n",
      "\t😍\t-3.5057201737618926\n",
      "\tgame\t-3.8414669655290274\n",
      "\tgirl\t-3.45728316126113\n",
      "\tseries\t-3.2755249579549517\n",
      "\tgames\t-3.201061851007929\n",
      "\tstyle\t-3.198356953227137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22137/3894624495.py:15: RuntimeWarning: divide by zero encountered in log2\n",
      "  pmis = np.log2(conditional_probs / total_term_probabilities)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Utah</td>\n",
       "      <td>[673.0000000000001, 540.0000000000003, 454.999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                   numpy_vectorized\n",
       "109      Utah  [673.0000000000001, 540.0000000000003, 454.999..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Popescul Ungars values for subreddit: Utah\n",
      "\tTerm\tPopescul Ungars\n",
      "\tplace\t0.01031517509504337\n",
      "\tone\t0.01128732533916695\n",
      "\tget\t0.01479576407431094\n",
      "\tlike\t0.015331136409848302\n",
      "\tcity\t0.033213616820400735\n",
      "\tstate\t0.06306228077298107\n",
      "\tpeople\t0.03391204994481017\n",
      "\tarea\t0.021341479994225122\n",
      "\tdrive\t0.01872680985526923\n",
      "\twater\t0.047482351064936965\n",
      "\n",
      "Bottom Popescul Ungars values for subreddit: Utah\n",
      "\tTerm\tPopescul Ungars\n",
      "\tcharacter\t1.7445821117245923e-05\n",
      "\tstyle\t5.577372746221594e-06\n",
      "\tfocus\t1.2771177735558381e-05\n",
      "\tplayers\t0.0\n",
      "\tplayer\t0.0\n",
      "\t😍\t4.507171891584577e-06\n",
      "\tvideos\t0.0\n",
      "\tseries\t7.930326080853805e-06\n",
      "\tgirl\t6.991596784306227e-06\n",
      "\tupdate\t1.160241827694808e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>MensRights</td>\n",
       "      <td>[4036.999999999999, 2821.000000000001, 2578.99...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                   numpy_vectorized\n",
       "72  MensRights  [4036.999999999999, 2821.000000000001, 2578.99..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top PMI values for subreddit: MensRights\n",
      "\tTerm\tPMI\n",
      "\thuman\t1.5442724450090743\n",
      "\tfact\t1.549393711560967\n",
      "\thealth\t1.5698787628567672\n",
      "\tman\t1.9367389193218343\n",
      "\tsimply\t1.740783431813583\n",
      "\tproblems\t1.8442367411991258\n",
      "\tsub\t1.5943485622675861\n",
      "\tsocial\t1.9574657618460578\n",
      "\tsupport\t2.002505995476581\n",
      "\tissues\t2.203955750760912\n",
      "\n",
      "Bottom PMI values for subreddit: MensRights\n",
      "\tTerm\tPMI\n",
      "\tupdate\t-4.398647429625132\n",
      "\tplayers\t-4.704745757574912\n",
      "\t😍\t-7.025812994425413\n",
      "\tplayer\t-4.231635416357287\n",
      "\tawesome\t-3.067739002895289\n",
      "\tseason\t-3.775615286435782\n",
      "\tplayed\t-2.891951128103457\n",
      "\tteam\t-2.679488571262257\n",
      "\tgame\t-3.179356454971799\n",
      "\tseries\t-2.7367240895649037\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>MensRights</td>\n",
       "      <td>[4036.999999999999, 2821.000000000001, 2578.99...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                   numpy_vectorized\n",
       "72  MensRights  [4036.999999999999, 2821.000000000001, 2578.99..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Popescul Ungars values for subreddit: MensRights\n",
      "\tTerm\tPopescul Ungars\n",
      "\tsaying\t0.011955155326173113\n",
      "\tsupport\t0.016843905542828642\n",
      "\tsay\t0.013064772694893331\n",
      "\tlike\t0.016765428445961128\n",
      "\tget\t0.012271820340296813\n",
      "\tthink\t0.012245987297997935\n",
      "\teven\t0.0174989201468032\n",
      "\tpeople\t0.030018883917453266\n",
      "\tman\t0.03493713952005967\n",
      "\tissues\t0.020375485604115375\n",
      "\n",
      "Bottom Popescul Ungars values for subreddit: MensRights\n",
      "\tTerm\tPopescul Ungars\n",
      "\tplayers\t1.5401053794964539e-06\n",
      "\tplayer\t2.3753539479429676e-06\n",
      "\t😍\t3.424499192543898e-08\n",
      "\tseason\t5.213436074682932e-06\n",
      "\tupdate\t1.2694159682930376e-06\n",
      "\tawesome\t1.17089714072208e-05\n",
      "\tstore\t1.5605222670340212e-05\n",
      "\tplayed\t1.5631002617460586e-05\n",
      "\tseries\t1.6737149341425625e-05\n",
      "\tdark\t1.8233094112020016e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>conservatives</td>\n",
       "      <td>[1035.9999999999998, 653.0, 654.9999999999999,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit                                   numpy_vectorized\n",
       "1206  conservatives  [1035.9999999999998, 653.0, 654.9999999999999,..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top PMI values for subreddit: conservatives\n",
      "\tTerm\tPMI\n",
      "\twon’t\t1.5002975096422868\n",
      "\tsocial\t1.506064032407606\n",
      "\tstupid\t1.6343570135500571\n",
      "\thistory\t1.6526291112332\n",
      "\tsays\t1.7984304039113945\n",
      "\twhite\t3.151878193587896\n",
      "\tblack\t2.5576171157772993\n",
      "\tstate\t2.5734764443005274\n",
      "\tcountry\t2.5642408476113876\n",
      "\tleft\t1.8515002940425347\n",
      "\n",
      "Bottom PMI values for subreddit: conservatives\n",
      "\tTerm\tPMI\n",
      "\thi\t-4.388907621467504\n",
      "\t😍\t-4.13004933747748\n",
      "\tseries\t-3.899854121670539\n",
      "\tseason\t-3.557923534600486\n",
      "\trecommend\t-3.3415389498994448\n",
      "\tplayer\t-3.3358717594093537\n",
      "\thello\t-3.1911359807718513\n",
      "\tplayers\t-3.171552180011687\n",
      "\top\t-2.6698150032244268\n",
      "\tamp;#x200b\t-2.5757192237714968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>conservatives</td>\n",
       "      <td>[1035.9999999999998, 653.0, 654.9999999999999,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit                                   numpy_vectorized\n",
       "1206  conservatives  [1035.9999999999998, 653.0, 654.9999999999999,..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Popescul Ungars values for subreddit: conservatives\n",
      "\tTerm\tPopescul Ungars\n",
      "\tsays\t0.013111929416861534\n",
      "\tright\t0.013291042136870785\n",
      "\tus\t0.016008405014019886\n",
      "\tleft\t0.018457254084803615\n",
      "\tcountry\t0.02641952104283382\n",
      "\tblack\t0.03509725626006695\n",
      "\twhite\t0.06892646543408866\n",
      "\tpeople\t0.06303906736373362\n",
      "\tstate\t0.02886261678190653\n",
      "\tlike\t0.01528903656470373\n",
      "\n",
      "Bottom Popescul Ungars values for subreddit: conservatives\n",
      "\tTerm\tPopescul Ungars\n",
      "\thello\t5.454473713112837e-06\n",
      "\thi\t1.5852450371166933e-06\n",
      "\tseries\t3.337393641708311e-06\n",
      "\t😍\t1.896795498658902e-06\n",
      "\trecommend\t6.552632093041161e-06\n",
      "\tseason\t7.049974513466076e-06\n",
      "\tplayer\t8.22302843213862e-06\n",
      "\tplayers\t1.2901046774814943e-05\n",
      "\tx\t1.7137806198255207e-05\n",
      "\top\t1.8266817506049435e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subreddit, numpy_vectorized]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top PMI values for subreddit: libertarianmemes\n",
      "\tTerm\tPMI\n",
      "\tgets\tnan\n",
      "\ttook\tnan\n",
      "\tagree\tnan\n",
      "\tpay\tnan\n",
      "\tcall\tnan\n",
      "\tfamily\tnan\n",
      "\tu\tnan\n",
      "\ttrue\tnan\n",
      "\ttold\tnan\n",
      "\twelcome\tnan\n",
      "\n",
      "Bottom PMI values for subreddit: libertarianmemes\n",
      "\tTerm\tPMI\n",
      "\tknew\tnan\n",
      "\tshot\tnan\n",
      "\thi\tnan\n",
      "\tanymore\tnan\n",
      "\tversion\tnan\n",
      "\tthats\tnan\n",
      "\thappens\tnan\n",
      "\tspace\tnan\n",
      "\tplus\tnan\n",
      "\tworked\tnan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22137/3894624495.py:4: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return token_count_array/total_tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numpy_vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subreddit, numpy_vectorized]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Popescul Ungars values for subreddit: libertarianmemes\n",
      "\tTerm\tPopescul Ungars\n",
      "\tgets\tnan\n",
      "\ttook\tnan\n",
      "\tagree\tnan\n",
      "\tpay\tnan\n",
      "\tcall\tnan\n",
      "\tfamily\tnan\n",
      "\tu\tnan\n",
      "\ttrue\tnan\n",
      "\ttold\tnan\n",
      "\twelcome\tnan\n",
      "\n",
      "Bottom Popescul Ungars values for subreddit: libertarianmemes\n",
      "\tTerm\tPopescul Ungars\n",
      "\tknew\tnan\n",
      "\tshot\tnan\n",
      "\thi\tnan\n",
      "\tanymore\tnan\n",
      "\tversion\tnan\n",
      "\tthats\tnan\n",
      "\thappens\tnan\n",
      "\tspace\tnan\n",
      "\tplus\tnan\n",
      "\tworked\tnan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22137/3894624495.py:4: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return token_count_array/total_tokens\n"
     ]
    }
   ],
   "source": [
    "selected_subreddits = [\"4chan\", \"Utah\", \"MensRights\", \"conservatives\", \"libertarianmemes\"]\n",
    "k = 10\n",
    "for s in selected_subreddits:\n",
    "    for (score_name, score_func) in [(\"PMI\", compute_pmi), (\"Popescul Ungars\", compute_differential_cluster_label_scheme)]:\n",
    " \n",
    "        selected_pdf = pandas_df[pandas_df[\"subreddit\"]==s]\n",
    "        display(selected_pdf)\n",
    "\n",
    "        pmi_values = score_func(selected_pdf, \"numpy_vectorized\", corpus_term_probabilities)\n",
    "\n",
    "        top_pmi_indices = np.argpartition(pmi_values, -k)[-k:]\n",
    "        print(\"Top\", score_name, \"values for subreddit:\", s)\n",
    "        print(f\"\\tTerm\\t{score_name}\")\n",
    "        for i in top_pmi_indices:\n",
    "            print(f\"\\t{id_to_term[i]}\\t{pmi_values[i]}\")\n",
    "        print()\n",
    "        bottom_pmi_indices = np.argpartition(pmi_values,k)[:k]\n",
    "        print(\"Bottom\", score_name, \"values for subreddit:\", s)\n",
    "        print(f\"\\tTerm\\t{score_name}\")\n",
    "        for i in bottom_pmi_indices:\n",
    "            print(f\"\\t{id_to_term[i]}\\t{pmi_values[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "645e6807a54f1780e2a687ae083eb2a94c3bf70f4809b5f746590e8ef179a45c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
