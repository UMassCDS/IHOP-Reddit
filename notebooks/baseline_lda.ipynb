{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LDA model\n",
    "This notebook gives an overview of how to train an LDA model from the Reddit data.\n",
    "\n",
    "The input is a joined submissions and commends dataframe as produced by `notebooks/bagOfWords_preprocessing_databricks.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/22 13:43:27 WARN Utils: Your hostname, Kurt resolves to a loopback address: 127.0.1.1; using 192.168.0.11 instead (on interface wlp4s0)\n",
      "22/02/22 13:43:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/22 13:43:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/02/22 13:43:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark configuration:\n",
      "[('spark.app.startTime', '1645555408216'), ('spark.driver.port', '35221'), ('spark.app.id', 'local-1645555410249'), ('spark.executor.id', 'driver'), ('spark.app.name', 'baseline lda'), ('spark.driver.memory', '8G'), ('spark.driver.host', '192.168.0.11'), ('spark.sql.warehouse.dir', 'file:/home/virginia/Documents/CenterForDataScience/ZuckermanProj/IHOP/notebooks/spark-warehouse'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.executor.extraLibraryPath', '/home/virginia/hadoop-3.3.1/lib/native'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.extraLibraryPath', '/home/virginia/hadoop-3.3.1/lib/native')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import ihop.utils\n",
    "spark = ihop.utils.get_spark_session(\"baseline lda\")\n",
    "\n",
    "input_data = spark.read.load(\n",
    "    \"../data/bagOfWords/2021-05_to_2021-06_joined_submissions_comments_5percentTopUsersExcludedFromComments_02102022.parquet\").limit(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>               (145 + 4) / 200]\r"
     ]
    }
   ],
   "source": [
    "input_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a simple LDA model using Gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import ihop.clustering as ic\n",
    "import ihop.text_processing as itp\n",
    "\n",
    "# Read in the joined data, collecting all the comments for each submission\n",
    "# Any desired filtering by time stamps can be done here\n",
    "corpus = itp.SparkCorpus.init_from_joined_dataframe(input_data)\n",
    "\n",
    "# Tokenize the document, then create an id to word index and vectorize each document\n",
    "# This is where you would set minimum and maximum document frequency and minimum term frequency, passed to Spark CountVectorizer\n",
    "pipeline = itp.SparkTextPreprocessingPipeline('document_text')\n",
    "transformed = pipeline.fit_transform(corpus.document_dataframe)\n",
    "vectorized_corpus = itp.SparkCorpus(transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pipeline.get_id_to_word()\n",
    "print(\"INDEX DETAILS:\")\n",
    "print(\"Vocab size:\", len(index))\n",
    "for k in range(10):\n",
    "    print(k, index[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus.document_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_iterator = vectorized_corpus.get_vectorized_column_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = ic.GensimLDAModel(corpus_iterator, \"sample_lda\", index, num_topics=10)\n",
    "\n",
    "print(\"Starting training at\", datetime.now())\n",
    "lda_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_top_words_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_top_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_term_topics('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_corpus_iter = vectorized_corpus.get_vectorized_column_iterator(use_id_col=True)\n",
    "doc_topics = lda_model.get_topic_assigments(id_corpus_iter)\n",
    "print(len(doc_topics))\n",
    "doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = corpus.document_dataframe.select('id', 'subreddit').toPandas()\n",
    "topics_df = lda_model.get_cluster_results_as_df(vocab_col_name=\"id\", join_df = subreddits)\n",
    "topics_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "645e6807a54f1780e2a687ae083eb2a94c3bf70f4809b5f746590e8ef179a45c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ihop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
